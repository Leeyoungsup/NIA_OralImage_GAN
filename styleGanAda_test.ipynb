{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from enum import Enum\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "import datetime\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "num_epochs = 50# train for 400 epochs for good results\n",
    "image_size = 256\n",
    "# resolution of Kernel Inception Distance measurement, see related section\n",
    "kid_image_size = 256\n",
    "padding = 0.25\n",
    "dataset_name = \"caltech_birds2011\"\n",
    "\n",
    "# adaptive discriminator augmentation\n",
    "max_translation = 0.125\n",
    "max_rotation = 0.125\n",
    "max_zoom = 0.25\n",
    "target_accuracy = 0.92\n",
    "integration_steps = 1000\n",
    "\n",
    "# architecture\n",
    "noise_size = 256\n",
    "depth = 6#64=4 128=5\n",
    "width = 512\n",
    "leaky_relu_slope = 0.2\n",
    "dropout_rate = 0.4\n",
    "\n",
    "# optimization\n",
    "batch_size = 16\n",
    "learning_rate = 2e-5\n",
    "beta_1 = 0.5  # not using the default value of 0.9 is important\n",
    "ema = 0.5\n",
    "#num_epochs: 전체 데이터셋에 대한 반복 횟수(epoch)\n",
    "#image_size: 생성할 이미지의 크기\n",
    "#kid_image_size: Kernel Inception Distance(KID) 측정 시 사용할 이미지 해상도\n",
    "#padding: 패딩 비율\n",
    "#dataset_name: 사용할 데이터셋 이름\n",
    "#max_translation: Adaptive Discriminator Augmentation(ADA)에서 사용될 최대 이동 비율\n",
    "#max_rotation: ADA에서 사용될 최대 회전 비율\n",
    "#max_zoom: ADA에서 사용될 최대 확대/축소 비율\n",
    "#target_accuracy: ADA에서 목표로 하는 정확도\n",
    "#integration_steps: ADA에서 사용될 적분 단계 수\n",
    "#noise_size: 생성기 입력으로 사용될 노이즈 벡터의 크기\n",
    "#depth: 생성기와 판별기에서 사용될 블록 수\n",
    "#width: 블록 내부의 뉴런 수\n",
    "#leaky_relu_slope: Leaky ReLU 함수에서 사용될 slope 값\n",
    "#dropout_rate: 블록 내부에서 사용될 드롭아웃 비율\n",
    "#batch_size: 미니배치 크기\n",
    "#learning_rate: 학습률\n",
    "#beta_1: Adam 옵티마이저의 decay rate beta_1\n",
    "#ema: Exponential Moving Average(EMA)에 사용될 decay rate. 이 값이 높을수록 더 많은 이전 스텝의 정보를 반영합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 훈련용 마스크 답지  및 병변여부 \n",
    "train_x_filenames = glob.glob('../data/d2/*.png')\n",
    "\n",
    "train_x = np.zeros((len(train_x_filenames),image_size,image_size,3),dtype=np.int32)\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "buffer_size = image_size\n",
    "\n",
    "def normalize_img(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    # Map values in the range [-1, 1]\n",
    "    return img/255  ############\n",
    "\n",
    "\n",
    "for (index, image) in enumerate(train_x_filenames[:]):\n",
    "    \n",
    "    img1=PIL.Image.open(image).convert(\"RGB\")\n",
    "    if(img1.size[0]>=img1.size[1]):\n",
    "        ratio=image_size/img1.size[0]\n",
    "    else:\n",
    "        ratio=image_size/img1.size[1]\n",
    "    width=ratio*img1.size[0]\n",
    "    height=ratio*img1.size[1]\n",
    "    height_padding_size=int((image_size-height)/2)\n",
    "    width_padding_size=int((image_size-width)/2)\n",
    "    img1=np.array(img1.resize((int(width),int(height))))\n",
    "    train_x[index,height_padding_size:-height_padding_size,:] = img1\n",
    "# 훈련용 마스크 답지  및 병변여부 \n",
    "tf_train_x=tf.data.Dataset.from_tensor_slices(train_x)\n",
    "\n",
    "tf_train_x=(tf_train_x.map(normalize_img, num_parallel_calls=autotune).cache()\n",
    "    .batch(batch_size)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KID(Kernel Inception Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KID(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"kid\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        # KID is estimated per batch and is averaged across batches\n",
    "        self.kid_tracker = keras.metrics.Mean()\n",
    "\n",
    "        # a pretrained InceptionV3 is used without its classification layer\n",
    "        # transform the pixel values to the 0-255 range, then use the same\n",
    "        # preprocessing as during pretraining\n",
    "        self.encoder = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                layers.Rescaling(255.0),\n",
    "                layers.Resizing(height=kid_image_size, width=kid_image_size),\n",
    "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
    "                keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(kid_image_size, kid_image_size, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n",
    "        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(\n",
    "            kernel_generated * (1.0 - tf.eye(batch_size))\n",
    "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive discriminator augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"hard sigmoid\", useful for binary accuracy calculation from logits\n",
    "def step(values):\n",
    "    # negative values -> 0.0, positive values -> 1.0\n",
    "    return 0.5 * (1.0 + tf.sign(values))\n",
    "\n",
    "\n",
    "# augments images with a probability that is dynamically updated during training\n",
    "class AdaptiveAugmenter(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # stores the current probability of an image being augmented\n",
    "        self.probability = tf.Variable(0.0)\n",
    "\n",
    "        # the corresponding augmentation names from the paper are shown above each layer\n",
    "        # the authors show (see figure 4), that the blitting and geometric augmentations\n",
    "        # are the most helpful in the low-data regime\n",
    "        self.augmenter = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                # blitting/x-flip:\n",
    "                layers.RandomFlip(\"horizontal\"),\n",
    "                # blitting/integer translation:\n",
    "                layers.RandomTranslation(\n",
    "                    height_factor=max_translation,\n",
    "                    width_factor=max_translation,\n",
    "                    interpolation=\"nearest\",\n",
    "                ),\n",
    "                # geometric/rotation:\n",
    "                layers.RandomRotation(factor=max_rotation),\n",
    "                # geometric/isotropic and anisotropic scaling:\n",
    "                layers.RandomZoom(\n",
    "                    height_factor=(-max_zoom, 0.0), width_factor=(-max_zoom, 0.0)\n",
    "                ),\n",
    "            ],\n",
    "            name=\"adaptive_augmenter\",\n",
    "        )\n",
    "\n",
    "    def call(self, images, training):\n",
    "        if training:\n",
    "            augmented_images = self.augmenter(images, training)\n",
    "\n",
    "            # during training either the original or the augmented images are selected\n",
    "            # based on self.probability\n",
    "            augmentation_values = tf.random.uniform(\n",
    "                shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "            )\n",
    "            augmentation_bools = tf.math.less(augmentation_values, self.probability)\n",
    "\n",
    "            images = tf.where(augmentation_bools, augmented_images, images)\n",
    "        return images\n",
    "\n",
    "    def update(self, real_logits):\n",
    "        current_accuracy = tf.reduce_mean(step(real_logits))\n",
    "\n",
    "        # the augmentation probability is updated based on the dicriminator's\n",
    "        # accuracy on real images\n",
    "        accuracy_error = current_accuracy - target_accuracy\n",
    "        self.probability.assign(\n",
    "            tf.clip_by_value(\n",
    "                self.probability + accuracy_error / integration_steps, 0.0, 1.0\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN generator\n",
    "def get_generator():\n",
    "    noise_input = keras.Input(shape=(noise_size,))\n",
    "    x = layers.Dense(4 * 4 * width, use_bias=False)(noise_input)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape(target_shape=(4, 4, width))(x)\n",
    "    for _ in range(depth - 1):\n",
    "        x = layers.Conv2DTranspose(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.ReLU()(x)\n",
    "    image_output = layers.Conv2DTranspose(\n",
    "        3, kernel_size=4, strides=2, padding=\"same\", activation=\"sigmoid\",\n",
    "    )(x)\n",
    "\n",
    "    return keras.Model(noise_input, image_output, name=\"generator\")\n",
    "\n",
    "\n",
    "# DCGAN discriminator\n",
    "def get_discriminator():\n",
    "    image_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    x = image_input\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.LeakyReLU(alpha=leaky_relu_slope)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    output_score = layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(image_input, output_score, name=\"discriminator\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GanModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_ADA(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.augmenter = AdaptiveAugmenter()\n",
    "        self.generator = get_generator()\n",
    "        self.ema_generator = keras.models.clone_model(self.generator)\n",
    "        self.discriminator = get_discriminator()\n",
    "\n",
    "    def compile(self, generator_optimizer, discriminator_optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        # separate optimizers for the two networks\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "\n",
    "        self.generator_loss_tracker = keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.discriminator_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.real_accuracy = keras.metrics.BinaryAccuracy(name=\"real_acc\")\n",
    "        self.generated_accuracy = keras.metrics.BinaryAccuracy(name=\"gen_acc\")\n",
    "        self.augmentation_probability_tracker = keras.metrics.Mean(name=\"aug_p\")\n",
    "        self.kid = KID()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.generator_loss_tracker,\n",
    "            self.discriminator_loss_tracker,\n",
    "            self.real_accuracy,\n",
    "            self.generated_accuracy,\n",
    "            self.augmentation_probability_tracker,\n",
    "            self.kid,\n",
    "        ]\n",
    "\n",
    "    def generate(self, batch_size, training):\n",
    "        latent_samples = tf.random.normal(shape=(batch_size, noise_size),seed=81)\n",
    "        # use ema_generator during inference\n",
    "        if training:\n",
    "            generated_images = self.generator(latent_samples, training)\n",
    "        else:\n",
    "            generated_images = self.ema_generator(latent_samples, training)\n",
    "        return generated_images\n",
    "\n",
    "    def adversarial_loss(self, real_logits, generated_logits):\n",
    "        # this is usually called the non-saturating GAN loss\n",
    "\n",
    "        real_labels = tf.ones(shape=(batch_size, 1))\n",
    "        generated_labels = tf.zeros(shape=(batch_size, 1))\n",
    "\n",
    "        # the generator tries to produce images that the discriminator considers as real\n",
    "        generator_loss = keras.losses.binary_crossentropy(\n",
    "            real_labels, generated_logits, from_logits=True\n",
    "        )\n",
    "        # the discriminator tries to determine if images are real or generated\n",
    "        discriminator_loss = keras.losses.binary_crossentropy(\n",
    "            tf.concat([real_labels, generated_labels], axis=0),\n",
    "            tf.concat([real_logits, generated_logits], axis=0),\n",
    "            from_logits=True,\n",
    "        )\n",
    "\n",
    "        return tf.reduce_mean(generator_loss), tf.reduce_mean(discriminator_loss)\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        real_images = self.augmenter(real_images, training=True)\n",
    "\n",
    "        # use persistent gradient tape because gradients will be calculated twice\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            generated_images = self.generate(batch_size, training=True)\n",
    "            # gradient is calculated through the image augmentation\n",
    "            generated_images = self.augmenter(generated_images, training=True)\n",
    "\n",
    "            # separate forward passes for the real and generated images, meaning\n",
    "            # that batch normalization is applied separately\n",
    "            real_logits = self.discriminator(real_images, training=True)\n",
    "            generated_logits = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            generator_loss, discriminator_loss = self.adversarial_loss(\n",
    "                real_logits, generated_logits\n",
    "            )\n",
    "\n",
    "        # calculate gradients and update weights\n",
    "        generator_gradients = tape.gradient(\n",
    "            generator_loss, self.generator.trainable_weights\n",
    "        )\n",
    "        discriminator_gradients = tape.gradient(\n",
    "            discriminator_loss, self.discriminator.trainable_weights\n",
    "        )\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, self.generator.trainable_weights)\n",
    "        )\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(discriminator_gradients, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # update the augmentation probability based on the discriminator's performance\n",
    "        self.augmenter.update(real_logits)\n",
    "\n",
    "        self.generator_loss_tracker.update_state(generator_loss)\n",
    "        self.discriminator_loss_tracker.update_state(discriminator_loss)\n",
    "        self.real_accuracy.update_state(1.0, step(real_logits))\n",
    "        self.generated_accuracy.update_state(0.0, step(generated_logits))\n",
    "        self.augmentation_probability_tracker.update_state(self.augmenter.probability)\n",
    "\n",
    "        # track the exponential moving average of the generator's weights to decrease\n",
    "        # variance in the generation quality\n",
    "        for weight, ema_weight in zip(\n",
    "            self.generator.weights, self.ema_generator.weights\n",
    "        ):\n",
    "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "\n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics[:-1]}\n",
    "\n",
    "    def test_step(self, real_images):\n",
    "        generated_images = self.generate(batch_size, training=False)\n",
    "\n",
    "        self.kid.update_state(real_images, generated_images)\n",
    "\n",
    "        # only KID is measured during the evaluation phase for computational efficiency\n",
    "        return {self.kid.name: self.kid.result()}\n",
    "\n",
    "    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6, interval=25):\n",
    "        # plot random generated images for visual evaluation of generation quality\n",
    "        if epoch is None or (epoch + 1) % interval == 0:\n",
    "            num_images = num_rows * num_cols\n",
    "            generated_images = self.generate(num_images, training=False)\n",
    "                    \n",
    "        return generated_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.training.tracking.util.CheckpointLoadStatus at 0x7f54a5887c70>"
      ]
     },
     "execution_count": 202,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = GAN_ADA()\n",
    "model.compile(\n",
    "    generator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    "    discriminator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    ")\n",
    "model.load_weights(\"../model/StyleGan/embryo_micro/model24.tf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.test_step(tf.convert_to_tensor(train_x[24:25]))\n",
    "img=model.plot_images(num_rows=15, num_cols=15)\n",
    "img=(img.numpy()*255).astype(np.int32)\n",
    "\n",
    "for i in range(len(img)):\n",
    "    cv2.imwrite(\"../data/d2/38/\"+str(i)+\".tiff\",img[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'kid': <tf.Tensor: shape=(), dtype=float32, numpy=nan>}"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(16, 256), dtype=float32, numpy=\n",
       "array([[-0.9685297 , -0.02737954, -1.2284228 , ..., -0.4842014 ,\n",
       "         1.343649  , -1.800174  ],\n",
       "       [-0.3203607 , -0.23615338,  1.6571171 , ..., -0.65236205,\n",
       "        -0.85498685,  2.2989514 ],\n",
       "       [-0.13040502,  0.14937118,  0.4730401 , ...,  2.3516083 ,\n",
       "         1.1476551 , -0.17996103],\n",
       "       ...,\n",
       "       [-0.4522002 ,  0.88092595,  1.0183833 , ...,  1.0404195 ,\n",
       "         1.5540863 , -2.51807   ],\n",
       "       [ 0.9611606 ,  0.2919494 , -0.5599798 , ..., -0.42612016,\n",
       "         0.0557483 , -2.074174  ],\n",
       "       [ 1.2741579 , -0.23100922, -1.3696119 , ..., -0.05350478,\n",
       "        -1.1043723 ,  0.6153486 ]], dtype=float32)>"
      ]
     },
     "execution_count": 160,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.random.normal(shape=(batch_size, noise_size))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ace6096bc4ec7c9763b7a7f8aba72601b4924b997cc8a285eb70b9b69117778c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
