{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/dgxadmin/anaconda3/envs/LeeYS/lib/python3.6/site-packages/tensorflow_addons/utils/ensure_tf_install.py:68: UserWarning: Tensorflow Addons supports using Python ops for all Tensorflow versions above or equal to 2.2.0 and strictly below 2.3.0 (nightly versions are not supported). \n",
      " The versions of TensorFlow you are currently using is 2.3.0 and is not supported. \n",
      "Some things might work, some things might not.\n",
      "If you were to encounter a bug, do not file an issue.\n",
      "If you want to make sure you're using a tested and supported configuration, either change the TensorFlow version or the TensorFlow Addons's version. \n",
      "You can find the compatibility matrix in TensorFlow Addon's readme:\n",
      "https://github.com/tensorflow/addons\n",
      "  UserWarning,\n"
     ]
    }
   ],
   "source": [
    "import slideio\n",
    "import json\n",
    "import matplotlib.path as mpltPath\n",
    "import warnings\n",
    "import tifffile\n",
    "import staintools\n",
    "from skimage import exposure\n",
    "from skimage.exposure import match_histograms\n",
    "import xml.etree.ElementTree as ET\n",
    "import pandas as pd\n",
    "from math import sqrt\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from numpy import cov\n",
    "from numpy import trace\n",
    "from numpy import iscomplexobj\n",
    "from numpy.random import random\n",
    "from scipy.linalg import sqrtm\n",
    "import cv2\n",
    "import glob\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as K\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow_examples.models.pix2pix import pix2pix\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "import tensorflow_addons as tfa\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import clear_output\n",
    "import PIL\n",
    "import PIL.Image\n",
    "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"3\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfds.disable_progress_bar()\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "def normalize_img(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    # Map values in the range [-1, 1]\n",
    "    return (img / 127.5) - 1.0\n",
    "\n",
    "def normalize_img(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    # Map values in the range [-1, 1]\n",
    "    return (img / 127.5) - 1.0\n",
    "\n",
    "\n",
    "def preprocess_train_image(img):\n",
    "    # Random flip\n",
    "    # Resize to the original size first\n",
    "    # Normalize the pixel values in the range [-1, 1]\n",
    "    img = normalize_img(img)\n",
    "    return img\n",
    "\n",
    "\n",
    "def preprocess_test_image(img, label):\n",
    "    # Only resizing and normalization for the test images.\n",
    "    img = tf.image.resize(img, [input_img_size[0], input_img_size[1]])\n",
    "    img = normalize_img(img)\n",
    "    return img\n",
    "\n",
    "input_img_size = (256,256, 3)\n",
    "\n",
    "orig_img_size = (286, 286)\n",
    "# Weights initializer for the layers.\n",
    "kernel_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "# Gamma initializer for instance normalization.\n",
    "gamma_init = keras.initializers.RandomNormal(mean=0.0, stddev=0.02)\n",
    "\n",
    "buffer_size = 256\n",
    "batch_size = 1\n",
    "# calculate frechet inception distance\n",
    "def calculate_fid(act1, act2):\n",
    "\t# calculate mean and covariance statistics\n",
    "\tmu1, sigma1 = act1.mean(axis=0), cov(act1, rowvar=False)\n",
    "\tmu2, sigma2 = act2.mean(axis=0), cov(act2, rowvar=False)\n",
    "\t# calculate sum squared difference between means\n",
    "\tssdiff = np.sum((mu1 - mu2)**2.0)\n",
    "\t# calculate sqrt of product between cov\n",
    "\tcovmean = sqrtm(sigma1.dot(sigma2))\n",
    "\t# check and correct imaginary numbers from sqrt\n",
    "\tif iscomplexobj(covmean):\n",
    "\t\tcovmean = covmean.real\n",
    "\t# calculate score\n",
    "\tfid = ssdiff + trace(sigma1 + sigma2 - 2.0 * covmean)\n",
    "\treturn fid\n",
    "\n",
    "class ReflectionPadding2D(layers.Layer):\n",
    "    \"\"\"Implements Reflection Padding as a layer.\n",
    "\n",
    "    Args:\n",
    "        padding(tuple): Amount of padding for the\n",
    "        spatial dimensions.\n",
    "\n",
    "    Returns:\n",
    "        A padded tensor with the same type as the input tensor.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, padding=(1, 1), **kwargs):\n",
    "        self.padding = tuple(padding)\n",
    "        super().__init__(**kwargs)\n",
    "\n",
    "    def call(self, input_tensor, mask=None):\n",
    "        padding_width, padding_height = self.padding\n",
    "        padding_tensor = [\n",
    "            [0, 0],\n",
    "            [padding_height, padding_height],\n",
    "            [padding_width, padding_width],\n",
    "            [0, 0],\n",
    "        ]\n",
    "        return tf.pad(input_tensor, padding_tensor, mode=\"REFLECT\")\n",
    "\n",
    "\n",
    "def residual_block(\n",
    "    x,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"valid\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    dim = x.shape[-1]\n",
    "    input_tensor = x\n",
    "\n",
    "    x = ReflectionPadding2D()(input_tensor)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = activation(x)\n",
    "\n",
    "    x = ReflectionPadding2D()(x)\n",
    "    x = layers.Conv2D(\n",
    "        dim,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.add([input_tensor, x])\n",
    "    return x\n",
    "\n",
    "\n",
    "def downsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_initializer=kernel_init,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        padding=padding,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def upsample(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(2, 2),\n",
    "    padding=\"same\",\n",
    "    kernel_initializer=kernel_init,\n",
    "    gamma_initializer=gamma_init,\n",
    "    use_bias=False,\n",
    "):\n",
    "    x = layers.Conv2DTranspose(\n",
    "        filters,\n",
    "        kernel_size,\n",
    "        strides=strides,\n",
    "        padding=padding,\n",
    "        kernel_initializer=kernel_initializer,\n",
    "        use_bias=use_bias,\n",
    "    )(x)\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    return x\n",
    "\n",
    "def get_resnet_generator(\n",
    "    filters=64,\n",
    "    num_downsampling_blocks=2,\n",
    "    num_residual_blocks=9,\n",
    "    num_upsample_blocks=2,\n",
    "    gamma_initializer=gamma_init,\n",
    "    name=None,\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(img_input)\n",
    "    x = layers.Conv2D(filters, (7, 7), kernel_initializer=kernel_init, use_bias=False)(\n",
    "        x\n",
    "    )\n",
    "    x = tfa.layers.InstanceNormalization(gamma_initializer=gamma_initializer)(x)\n",
    "    x = layers.Activation(\"relu\")(x)\n",
    "\n",
    "    # Downsampling\n",
    "    for _ in range(num_downsampling_blocks):\n",
    "        filters *= 2\n",
    "        x = downsample(x, filters=filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Residual blocks\n",
    "    for _ in range(num_residual_blocks):\n",
    "        x = residual_block(x, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Upsampling\n",
    "    for _ in range(num_upsample_blocks):\n",
    "        filters //= 2\n",
    "        x = upsample(x, filters, activation=layers.Activation(\"relu\"))\n",
    "\n",
    "    # Final block\n",
    "    x = ReflectionPadding2D(padding=(3, 3))(x)\n",
    "    x = layers.Conv2D(3, (7, 7), padding=\"valid\")(x)\n",
    "    x = layers.Activation(\"tanh\")(x)\n",
    "\n",
    "    model = keras.models.Model(img_input, x, name=name)\n",
    "    return model\n",
    "\n",
    "def get_discriminator(\n",
    "    filters=64, kernel_initializer=kernel_init, num_downsampling=3, name=None\n",
    "):\n",
    "    img_input = layers.Input(shape=input_img_size, name=name + \"_img_input\")\n",
    "    x = layers.Conv2D(\n",
    "        filters,\n",
    "        (4, 4),\n",
    "        strides=(2, 2),\n",
    "        padding=\"same\",\n",
    "        kernel_initializer=kernel_initializer,\n",
    "    )(img_input)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    num_filters = filters\n",
    "    for num_downsample_block in range(3):\n",
    "        num_filters *= 2\n",
    "        if num_downsample_block < 2:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(2, 2),\n",
    "            )\n",
    "        else:\n",
    "            x = downsample(\n",
    "                x,\n",
    "                filters=num_filters,\n",
    "                activation=layers.LeakyReLU(0.2),\n",
    "                kernel_size=(4, 4),\n",
    "                strides=(1, 1),\n",
    "            )\n",
    "\n",
    "    x = layers.Conv2D(\n",
    "        1, (4, 4), strides=(1, 1), padding=\"same\", kernel_initializer=kernel_initializer\n",
    "    )(x)\n",
    "\n",
    "    model = keras.models.Model(inputs=img_input, outputs=x, name=name)\n",
    "    return model\n",
    "\n",
    "\n",
    "# Get the generators\n",
    "gen_G = get_resnet_generator(name=\"generator_G\")\n",
    "gen_F = get_resnet_generator(name=\"generator_F\")\n",
    "\n",
    "# Get the discriminators\n",
    "disc_X = get_discriminator(name=\"discriminator_X\")\n",
    "disc_Y = get_discriminator(name=\"discriminator_Y\")\n",
    "\n",
    "class CycleGan(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        generator_G,\n",
    "        generator_F,\n",
    "        discriminator_X,\n",
    "        discriminator_Y,\n",
    "        lambda_cycle=10.0,\n",
    "        lambda_identity=0.5,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.gen_G = generator_G\n",
    "        self.gen_F = generator_F\n",
    "        self.disc_X = discriminator_X\n",
    "        self.disc_Y = discriminator_Y\n",
    "        self.lambda_cycle = lambda_cycle\n",
    "        self.lambda_identity = lambda_identity\n",
    "\n",
    "    def compile(\n",
    "        self,\n",
    "        gen_G_optimizer,\n",
    "        gen_F_optimizer,\n",
    "        disc_X_optimizer,\n",
    "        disc_Y_optimizer,\n",
    "        gen_loss_fn,\n",
    "        disc_loss_fn,\n",
    "    ):\n",
    "        super().compile()\n",
    "        self.gen_G_optimizer = gen_G_optimizer\n",
    "        self.gen_F_optimizer = gen_F_optimizer\n",
    "        self.disc_X_optimizer = disc_X_optimizer\n",
    "        self.disc_Y_optimizer = disc_Y_optimizer\n",
    "        self.generator_loss_fn = gen_loss_fn\n",
    "        self.discriminator_loss_fn = disc_loss_fn\n",
    "        self.cycle_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "        self.identity_loss_fn = keras.losses.MeanAbsoluteError()\n",
    "\n",
    "    def train_step(self, batch_data):\n",
    "        # x is Horse and y is zebra\n",
    "        real_x, real_y = batch_data\n",
    "\n",
    "        # For CycleGAN, we need to calculate different\n",
    "        # kinds of losses for the generators and discriminators.\n",
    "        # We will perform the following steps here:\n",
    "        #\n",
    "        # 1. Pass real images through the generators and get the generated images\n",
    "        # 2. Pass the generated images back to the generators to check if we\n",
    "        #    we can predict the original image from the generated image.\n",
    "        # 3. Do an identity mapping of the real images using the generators.\n",
    "        # 4. Pass the generated images in 1) to the corresponding discriminators.\n",
    "        # 5. Calculate the generators total loss (adverserial + cycle + identity)\n",
    "        # 6. Calculate the discriminators loss\n",
    "        # 7. Update the weights of the generators\n",
    "        # 8. Update the weights of the discriminators\n",
    "        # 9. Return the losses in a dictionary\n",
    "\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            # Horse to fake zebra\n",
    "            fake_y = self.gen_G(real_x, training=True)\n",
    "            # Zebra to fake horse -> y2x\n",
    "            fake_x = self.gen_F(real_y, training=True)\n",
    "\n",
    "            # Cycle (Horse to fake zebra to fake horse): x -> y -> x\n",
    "            cycled_x = self.gen_F(fake_y, training=True)\n",
    "            # Cycle (Zebra to fake horse to fake zebra) y -> x -> y\n",
    "            cycled_y = self.gen_G(fake_x, training=True)\n",
    "\n",
    "            # Identity mapping\n",
    "            same_x = self.gen_F(real_x, training=True)\n",
    "            same_y = self.gen_G(real_y, training=True)\n",
    "\n",
    "            # Discriminator output\n",
    "            disc_real_x = self.disc_X(real_x, training=True)\n",
    "            disc_fake_x = self.disc_X(fake_x, training=True)\n",
    "\n",
    "            disc_real_y = self.disc_Y(real_y, training=True)\n",
    "            disc_fake_y = self.disc_Y(fake_y, training=True)\n",
    "\n",
    "            # Generator adverserial loss\n",
    "            gen_G_loss = self.generator_loss_fn(disc_fake_y)\n",
    "            gen_F_loss = self.generator_loss_fn(disc_fake_x)\n",
    "\n",
    "            # Generator cycle loss\n",
    "            cycle_loss_G = self.cycle_loss_fn(real_y, cycled_y) * self.lambda_cycle\n",
    "            cycle_loss_F = self.cycle_loss_fn(real_x, cycled_x) * self.lambda_cycle\n",
    "\n",
    "            # Generator identity loss\n",
    "            id_loss_G = (\n",
    "                self.identity_loss_fn(real_y, same_y)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "            id_loss_F = (\n",
    "                self.identity_loss_fn(real_x, same_x)\n",
    "                * self.lambda_cycle\n",
    "                * self.lambda_identity\n",
    "            )\n",
    "\n",
    "            # Total generator loss\n",
    "            total_loss_G = gen_G_loss + cycle_loss_G + id_loss_G\n",
    "            total_loss_F = gen_F_loss + cycle_loss_F + id_loss_F\n",
    "\n",
    "            # Discriminator loss\n",
    "            disc_X_loss = self.discriminator_loss_fn(disc_real_x, disc_fake_x)\n",
    "            disc_Y_loss = self.discriminator_loss_fn(disc_real_y, disc_fake_y)\n",
    "\n",
    "        # Get the gradients for the generators\n",
    "        grads_G = tape.gradient(total_loss_G, self.gen_G.trainable_variables)\n",
    "        grads_F = tape.gradient(total_loss_F, self.gen_F.trainable_variables)\n",
    "\n",
    "        # Get the gradients for the discriminators\n",
    "        disc_X_grads = tape.gradient(disc_X_loss, self.disc_X.trainable_variables)\n",
    "        disc_Y_grads = tape.gradient(disc_Y_loss, self.disc_Y.trainable_variables)\n",
    "\n",
    "        # Update the weights of the generators\n",
    "        self.gen_G_optimizer.apply_gradients(\n",
    "            zip(grads_G, self.gen_G.trainable_variables)\n",
    "        )\n",
    "        self.gen_F_optimizer.apply_gradients(\n",
    "            zip(grads_F, self.gen_F.trainable_variables)\n",
    "        )\n",
    "\n",
    "        # Update the weights of the discriminators\n",
    "        self.disc_X_optimizer.apply_gradients(\n",
    "            zip(disc_X_grads, self.disc_X.trainable_variables)\n",
    "        )\n",
    "        self.disc_Y_optimizer.apply_gradients(\n",
    "            zip(disc_Y_grads, self.disc_Y.trainable_variables)\n",
    "        )\n",
    "\n",
    "        return {\n",
    "            \"G_loss\": total_loss_G,\n",
    "            \"F_loss\": total_loss_F,\n",
    "            \"D_X_loss\": disc_X_loss,\n",
    "            \"D_Y_loss\": disc_Y_loss,\n",
    "        }\n",
    "adv_loss_fn = keras.losses.MeanSquaredError()\n",
    "\n",
    "# Define the loss function for the generators\n",
    "def generator_loss_fn(fake):\n",
    "    fake_loss = adv_loss_fn(tf.ones_like(fake), fake)\n",
    "    return fake_loss\n",
    "\n",
    "\n",
    "# Define the loss function for the discriminators\n",
    "def discriminator_loss_fn(real, fake):\n",
    "    real_loss = adv_loss_fn(tf.ones_like(real), real)\n",
    "    fake_loss = adv_loss_fn(tf.zeros_like(fake), fake)\n",
    "    return (real_loss + fake_loss) * 0.5\n",
    "cycle_gan_model = CycleGan(\n",
    "        generator_G=gen_G, generator_F=gen_F, discriminator_X=disc_X, discriminator_Y=disc_Y\n",
    "    )\n",
    "cycle_gan_model.compile(\n",
    "        gen_G_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "        gen_F_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "        disc_X_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "        disc_Y_optimizer=keras.optimizers.Adam(learning_rate=2e-4, beta_1=0.5),\n",
    "        gen_loss_fn=generator_loss_fn,\n",
    "        disc_loss_fn=discriminator_loss_fn,\n",
    "    )\n",
    "cycle_gan_model.built = True\n",
    "cycle_gan_model.load_weights(\"../model/CycleGan/entire/cyclegan_checkpoints_070.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "MMRstatus_data=pd.read_csv('../MMRstatus.csv',encoding='cp949')\n",
    "P53_data=pd.read_csv('../P53.csv',encoding='cp949')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(action='ignore')\n",
    "files = os.listdir('../svs_data/')\n",
    "svs_files=[]\n",
    "xml_files=[]\n",
    "for i in range(len(files)):\n",
    "    svs_class=files[i].find('.svs')\n",
    "    xml_class=files[i].find('.xml')\n",
    "    if svs_class!=-1:\n",
    "        svs_files.append(files[i])\n",
    "    elif xml_class!=-1:\n",
    "        xml_files.append(files[i])\n",
    "def createDirectory(directory):\n",
    "    try:\n",
    "        if not os.path.exists(directory):\n",
    "            os.makedirs(directory)\n",
    "    except OSError:\n",
    "        print(\"Error: Failed to create the directory.\")   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "S10-01345-1\n",
      "S10-06438-2\n",
      "S10-06438-16\n",
      "S10-07753-6\n",
      "S10-27147-4R\n",
      "S10-27147-5R\n",
      "S10-28950-7\n",
      "S10-28950-8\n",
      "S11-05332-14\n",
      "S11-13522-9\n",
      "S11-14059-1\n",
      "S11-15498-9\n",
      "S11-15498-10\n",
      "S11-18614-2\n",
      "S11-23853-MG4\n",
      "S11-25637-12\n",
      "S11-25637-15\n",
      "S11-27049-1\n",
      "S11-27049-2\n",
      "S11-27296-2\n",
      "S11-27296-3\n",
      "S11-27296-4\n",
      "S11-27749-15\n",
      "S11-31465-15\n",
      "S11-33397-4\n",
      "S11-33397-8R\n",
      "S12-02539-7\n",
      "S12-05129-1\n",
      "S12-05129-9\n",
      "S12-15923-MG8\n",
      "S12-18293-4\n",
      "S12-25754-12\n",
      "S12-26702-1\n",
      "S12-30332-16\n",
      "S12-30332-17\n",
      "S13-16862-7\n",
      "S13-17955-3\n",
      "S13-17955-11\n",
      "S13-17955-12\n",
      "S13-17955-15\n",
      "S13-19937-23\n",
      "S13-23875-16R\n",
      "S14-04726-2\n",
      "S14-04976-5\n",
      "S14-04976-8\n",
      "S14-05313-4\n",
      "S14-05313-5\n",
      "S14-06111-6\n",
      "S14-11763-1\n",
      "S14-11763-2\n",
      "S14-12060-8\n",
      "S14-13997-2\n",
      "S14-13997-MG2\n",
      "S14-21094-2\n",
      "S14-21094-3\n",
      "S14-27557-5\n",
      "S14-29043-3\n",
      "S14-29043-4\n",
      "S15-01797-1\n",
      "S15-02984-36\n",
      "S15-03278-7\n",
      "S15-03278-8\n",
      "S15-03278-9\n",
      "S15-03845-8\n",
      "S15-04670-1\n",
      "S15-06063-2\n",
      "S15-14451-2\n",
      "S15-14451-5\n",
      "S15-14532-MR3\n",
      "S15-15513-4\n",
      "S15-15883-1\n",
      "S15-15883-27\n",
      "S15-15883-28\n",
      "S15-16222-7\n",
      "S15-18047-2R\n",
      "S15-25474-14\n",
      "S15-25474-18\n",
      "S15-25474-19\n",
      "S16-04113-10\n",
      "S16-04113-14\n",
      "S16-06596-10\n",
      "S16-07528-3\n",
      "S16-08284-11\n",
      "S16-09007-10\n",
      "S16-09342-4\n",
      "S16-10297-15\n",
      "S16-11048-7\n",
      "S16-13007-12\n",
      "S16-13063-11\n",
      "S16-13989-10\n",
      "S16-18617-2R\n",
      "S16-18617-4\n",
      "S16-18617-8R\n",
      "S16-29861-1R\n",
      "S16-31691-3R\n",
      "S16-34098-7\n",
      "S16-34565-3\n",
      "S17-01129-6\n",
      "S17-03589-4\n",
      "S17-08023-MG2\n",
      "S17-10762-1\n",
      "S17-10762-2\n",
      "S17-10797-MG3\n",
      "S17-14426-3\n",
      "S17-14426-4\n",
      "S17-14718-9\n",
      "S17-15254-4\n",
      "S17-15502-5\n",
      "S17-15867-3\n",
      "S17-15867-13\n",
      "S17-15867-16\n",
      "S17-22185-8\n",
      "S17-24907-13\n",
      "S17-24907-17\n",
      "S17-26735-3\n",
      "S17-27175-7\n",
      "S17-31741-4\n",
      "S17-31741-6\n",
      "S17-32155-5\n",
      "S17-33780-1\n",
      "S17-35228-1\n",
      "S17-35228-3\n",
      "S18-02693-2\n",
      "S18-02836-7\n",
      "S18-02840-4\n",
      "S18-02840-6\n",
      "S18-03134-3\n",
      "S18-03134-6\n",
      "S18-04157-7\n",
      "S18-05617-4\n",
      "S18-06033-2\n",
      "S18-06033-MG13\n",
      "S18-10844-11R\n",
      "S18-12652-6\n",
      "S18-12656-8R\n",
      "S18-14670-15\n",
      "S18-16347-2R\n",
      "S18-16347-8\n",
      "S18-21083-MG9\n",
      "S18-21083-MG10\n",
      "S18-28549-2\n",
      "S19-02716-27\n",
      "S19-04491-12\n",
      "S19-06858-5\n",
      "S19-07085-1\n",
      "S19-10741-19\n",
      "S19-15086-36\n",
      "S19-22218-11\n",
      "S19-23884-5\n",
      "S20-04323-11\n",
      "S20-07219-9\n",
      "S20-17152-5\n",
      "S21-00776-3\n",
      "S21-01502-8\n",
      "S21-01502-9\n",
      "S21-01502-11\n",
      "S21-01603-4R\n",
      "S21-01603-9\n",
      "S21-02295-9\n",
      "S21-03561-7\n",
      "S21-04189-2\n",
      "S21-04189-3\n",
      "S21-04189-5\n",
      "S21-06192-9\n",
      "S21-06192-28R\n",
      "S21-07239-9\n",
      "S21-08618-3\n",
      "S21-08618-4\n",
      "S21-10473-5R\n",
      "S21-12872-3\n",
      "S21-12872-4\n",
      "S21-13838-10R\n",
      "S21-24525-5\n",
      "S21-24525-13\n",
      "S21-24525-15R\n",
      "S21-24525-22\n",
      "S21-24525-25\n",
      "S21-24525-26\n",
      "S21-25244-1\n",
      "S21-25244-3\n",
      "S21-25244-4\n",
      "S21-25259-1R\n",
      "S21-25259-3\n",
      "S21-25259-5R\n",
      "S21-25930-6\n",
      "S21-25930-9\n",
      "S21-26531-4\n",
      "S21-26531-6\n",
      "S21-26531-9\n",
      "S21-27505-2\n",
      "S21-27505-3\n",
      "S21-27505-5\n",
      "S21-27505-17\n",
      "S21-29532-7\n",
      "S21-29532-MG3\n",
      "S21-29532-MG4\n",
      "S21-29642-7\n",
      "S21-29642-8\n",
      "S21-29642-9\n",
      "S21-29642-12\n",
      "S21-29690-3\n",
      "S21-29690-4R\n",
      "S21-29690-8\n",
      "S21-30016-2R\n",
      "S21-30016-11\n",
      "S21-30016-12\n",
      "S21-30467-2R\n",
      "S21-30506-1\n",
      "S21-30506-5\n",
      "S21-30506-6\n",
      "S21-30506-17R\n",
      "S21-30506-20R\n",
      "S21-30510-1\n",
      "S21-30510-10\n",
      "S21-30510-12\n",
      "S21-31566-3\n",
      "S21-31566-6R\n",
      "S21-31566-13\n",
      "S21-33968-8\n",
      "S21-33968-15\n",
      "S21-33968-17\n",
      "S21-33968-18\n",
      "S21-34345-18\n",
      "S22-00774-2\n",
      "S22-00774-4\n",
      "S22-00774-5\n",
      "S22-01177-5\n",
      "S22-01177-9\n",
      "S22-01177-11\n",
      "S22-01177-16\n",
      "S22-01738-5\n",
      "S22-01738-7\n",
      "S22-01738-9\n",
      "S22-02375-8\n",
      "S22-02375-10\n",
      "S22-02375-11\n",
      "S22-02378-11\n",
      "S22-02378-14\n",
      "S22-02378-18\n",
      "S22-02378-20\n",
      "S22-02722-5\n",
      "S22-02722-7\n",
      "S22-02782-9\n",
      "S22-02782-14R\n",
      "S22-02782-18\n",
      "S22-03684-4\n",
      "S22-03684-10\n",
      "S22-04989-3\n",
      "S22-04989-9\n",
      "S22-04989-11R\n",
      "S22-04989-13\n",
      "S22-04989-14\n",
      "S22-05305-16\n",
      "S22-05305-16R\n",
      "S22-05305-24\n",
      "S22-05305-25\n",
      "S22-05305-26R\n",
      "S22-05366-13R\n",
      "S22-05366-16\n",
      "S22-05366-19R\n",
      "S22-05570-9\n",
      "S22-05570-12R\n",
      "S22-05570-13R\n",
      "S22-05573-3\n",
      "S22-05573-6R\n",
      "S22-05573-7R\n",
      "S22-05573-12\n",
      "S22-05693-3\n",
      "S22-05693-5\n",
      "S22-05693-6\n",
      "S22-05693-31\n",
      "S22-05865-1\n",
      "S22-05865-2\n",
      "S22-05865-3\n",
      "S22-05865-4\n",
      "S22-07240-4\n",
      "S22-07240-5\n",
      "S22-07240-8\n",
      "S22-07243-5\n",
      "S22-07243-6\n",
      "S22-07243-7R\n",
      "S22-08675-1R\n",
      "S22-08675-2\n",
      "S22-08675-4R\n",
      "S22-11025-3\n",
      "S22-11025-4\n",
      "S22-11025-6\n",
      "S22-11101-15\n",
      "S22-11101-17\n",
      "S22-11101-20\n",
      "S22-11109-3R\n",
      "S22-11109-9\n",
      "S22-11109-22R\n",
      "S22-11314-3\n",
      "S22-11314-7\n",
      "S22-11314-10\n",
      "S22-11314-12\n",
      "S22-11647-3\n",
      "S22-11647-5\n",
      "S22-11647-8\n",
      "S22-11897-1\n",
      "S22-11897-3\n",
      "S22-11897-6\n",
      "S22-11897-23R\n",
      "S22-13220-18\n",
      "S22-13220-19\n",
      "S22-13220-22\n",
      "S22-13408-5\n",
      "S22-13408-6\n",
      "S22-13408-8\n",
      "S22-13408-16\n",
      "S22-13408-17\n",
      "S22-13676-1\n",
      "S22-13676-2\n",
      "S22-13676-3\n",
      "S22-14378-1\n",
      "S22-14389-1\n",
      "S22-14389-7\n",
      "S22-14389-9\n",
      "S22-14885-6R\n",
      "S22-14885-7R\n",
      "S22-14885-9R\n",
      "S22-15201-1R\n",
      "S22-15201-3R\n",
      "S22-15201-6\n",
      "S22-15572-7\n",
      "S22-15572-17\n",
      "S22-15572-24\n",
      "S22-16256-11\n",
      "S22-16256-13\n",
      "S22-16256-15\n",
      "S22-16733-4\n",
      "S22-16733-5\n",
      "S22-16733-8\n",
      "S22-17060-4\n",
      "S22-17060-5\n",
      "S22-17060-6\n",
      "S22-17060-7\n",
      "S22-18109-9\n",
      "S22-18109-10\n",
      "S22-19380-3\n",
      "S22-19380-4\n",
      "S22-19380-5\n",
      "S22-19730-7\n",
      "S22-19730-12\n",
      "S22-19730-19\n",
      "S22-19946-4\n",
      "S22-19946-7R\n",
      "S22-19946-9\n",
      "S22-20342-6R\n",
      "S22-20342-7\n",
      "S22-20342-13R\n",
      "S22-21614-8\n",
      "S22-21614-9R\n",
      "S22-21614-10R\n",
      "S22-21996-7R\n",
      "S22-21996-9\n",
      "S22-21996-14R\n",
      "S22-21996-24\n",
      "S22-23041-1R\n",
      "S22-23041-2R\n",
      "S22-23041-3\n",
      "S22-23041-6R\n",
      "S22-23041-12R\n",
      "S22-23041-17\n",
      "S22-23041-18\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for i in range(len(svs_files)):\n",
    "    try:\n",
    "        size=512\n",
    "        file_extension_index=svs_files[i].find('.svs')\n",
    "        fileName=svs_files[i][:file_extension_index]\n",
    "        slideName=svs_files[i][:9]\n",
    "        slide = slideio.open_slide('../svs_data/'+fileName+'.svs', \"SVS\")\n",
    "        scene = slide.get_scene(0)\n",
    "        svsWidth = scene.rect[2]\n",
    "        svsHeight = scene.rect[3]\n",
    "        widthRatio=size/svsWidth\n",
    "        heightRatio=size/svsHeight \n",
    "        imageCount=0\n",
    "        magnification=80\n",
    "        tile_size=256\n",
    "        if(svsHeight>svsWidth):\n",
    "            RatioMax=heightRatio\n",
    "            RatioIndex=\"H\"\n",
    "        else:\n",
    "            RatioMax=widthRatio\n",
    "            RatioIndex=\"W\"\n",
    "        xml_open='../svs_data/'+fileName+'.xml'\n",
    "        polygon_xml = ET.parse(xml_open)\n",
    "        root = polygon_xml.getroot()\n",
    "        root = root.getchildren()\n",
    "        root =root[0].getchildren()\n",
    "        imageCount=0\n",
    "        mask1 = np.zeros((size,size),dtype = np.uint8)\n",
    "        dstImg = np.ones((size,size,3),dtype = np.uint8)*255\n",
    "        path=[]\n",
    "        dstWidth=int(svsWidth*RatioMax)\n",
    "        dstHeight=int(svsHeight*RatioMax)\n",
    "        for annotation_count in range(len(root)):\n",
    "            root1 =root[annotation_count].getchildren()\n",
    "            root1 =root1[0].getchildren()\n",
    "            polygon=[]\n",
    "            for polygon_count in range(len(root1)): \n",
    "                polygon.append([int(float(root1[polygon_count].attrib['X'])),int(float(root1[polygon_count].attrib['Y']))])\n",
    "            polygon=np.array(polygon)\n",
    "            if(RatioIndex=='H'):\n",
    "                polygon[:,0]=(size-dstWidth)/2+polygon[:,0]*RatioMax\n",
    "                polygon[:,1]=polygon[:,1]*RatioMax\n",
    "            else:\n",
    "                polygon[:,0]=polygon[:,0]*RatioMax\n",
    "                polygon[:,1]=(size-dstHeight)/2+polygon[:,1]*RatioMax\n",
    "            path.append(np.array(polygon))\n",
    "        cv2.fillPoly(mask1,path,255)\n",
    "        thumbnailImg=scene.read_block((0,0,svsWidth,svsHeight),size=(dstWidth,dstHeight ))\n",
    "        if(RatioIndex=='H'):\n",
    "            dstImg[:,int((size-dstWidth)/2):int((size+dstWidth)/2)]=thumbnailImg\n",
    "        else:\n",
    "            dstImg[int((size-dstHeight)/2):int((size+dstHeight)/2),:]=thumbnailImg    \n",
    "\n",
    "        dstImg=np.reshape(dstImg,(1,size,size,3))\n",
    "        dstImg=tf.convert_to_tensor(dstImg)\n",
    "        dstImg=cycle_gan_model.gen_G(dstImg)[0].numpy()\n",
    "        dstImg=(dstImg*127.5+127.5).astype(np.uint8)\n",
    "        file_extension_index=svs_files[i].find('.svs')\n",
    "        dstImg=cv2.cvtColor(dstImg,cv2.COLOR_BGR2RGB)\n",
    "        fileName=svs_files[i][:file_extension_index]\n",
    "        cv2.imwrite('../data/CycleGanThumbnail/image/'+fileName+'.tiff',dstImg)\n",
    "        cv2.imwrite('../data/CycleGanThumbnail/mask/'+fileName+'.tiff',mask1)\n",
    "    except:\n",
    "        file_extension_index=svs_files[i].find('.svs')\n",
    "        fileName=svs_files[i][:file_extension_index]\n",
    "        print(fileName)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
