{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "import math\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from enum import Enum\n",
    "from glob import glob\n",
    "from functools import partial\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras import initializers, regularizers, constraints\n",
    "from tensorflow.keras.layers import Layer, InputSpec\n",
    "from tensorflow.python.keras.utils import conv_utils\n",
    "import PIL\n",
    "import PIL.Image\n",
    "import gdown\n",
    "from zipfile import ZipFile\n",
    "import datetime\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import cv2\n",
    "from tensorflow import keras\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# params\n",
    "num_epochs = 50# train for 400 epochs for good results\n",
    "image_size = 512\n",
    "# resolution of Kernel Inception Distance measurement, see related section\n",
    "kid_image_size = 640\n",
    "padding = 0.25\n",
    "dataset_name = \"caltech_birds2011\"\n",
    "\n",
    "# adaptive discriminator augmentation\n",
    "max_translation = 0.125\n",
    "max_rotation = 0.125\n",
    "max_zoom = 0.25\n",
    "target_accuracy = 0.92\n",
    "integration_steps = 1000\n",
    "\n",
    "# architecture\n",
    "noise_size = 512\n",
    "depth = 7#64=4 128=5\n",
    "width = 128\n",
    "leaky_relu_slope = 0.2\n",
    "dropout_rate = 0.4\n",
    "\n",
    "# optimization\n",
    "batch_size = 32\n",
    "learning_rate = 2e-5\n",
    "beta_1 = 0.5  # not using the default value of 0.9 is important\n",
    "ema = 0.99\n",
    "#num_epochs: 전체 데이터셋에 대한 반복 횟수(epoch)\n",
    "#image_size: 생성할 이미지의 크기\n",
    "#kid_image_size: Kernel Inception Distance(KID) 측정 시 사용할 이미지 해상도\n",
    "#padding: 패딩 비율\n",
    "#dataset_name: 사용할 데이터셋 이름\n",
    "#max_translation: Adaptive Discriminator Augmentation(ADA)에서 사용될 최대 이동 비율\n",
    "#max_rotation: ADA에서 사용될 최대 회전 비율\n",
    "#max_zoom: ADA에서 사용될 최대 확대/축소 비율\n",
    "#target_accuracy: ADA에서 목표로 하는 정확도\n",
    "#integration_steps: ADA에서 사용될 적분 단계 수\n",
    "#noise_size: 생성기 입력으로 사용될 노이즈 벡터의 크기\n",
    "#depth: 생성기와 판별기에서 사용될 블록 수\n",
    "#width: 블록 내부의 뉴런 수\n",
    "#leaky_relu_slope: Leaky ReLU 함수에서 사용될 slope 값\n",
    "#dropout_rate: 블록 내부에서 사용될 드롭아웃 비율\n",
    "#batch_size: 미니배치 크기\n",
    "#learning_rate: 학습률\n",
    "#beta_1: Adam 옵티마이저의 decay rate beta_1\n",
    "#ema: Exponential Moving Average(EMA)에 사용될 decay rate. 이 값이 높을수록 더 많은 이전 스텝의 정보를 반영합니다."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 훈련용 마스크 답지  및 병변여부 \n",
    "train_x_filenames = glob.glob('../data/d2/*.png')\n",
    "\n",
    "train_x = np.zeros((len(train_x_filenames),image_size,image_size,3),dtype=np.int32)\n",
    "autotune = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "\n",
    "buffer_size = image_size\n",
    "\n",
    "def normalize_img(img):\n",
    "    img = tf.cast(img, dtype=tf.float32)\n",
    "    # Map values in the range [-1, 1]\n",
    "    return img/255  ############\n",
    "\n",
    "\n",
    "for (index, image) in enumerate(train_x_filenames[:]):\n",
    "    \n",
    "    img1=PIL.Image.open(image).convert(\"RGB\")\n",
    "    if(img1.size[0]>=img1.size[1]):\n",
    "        ratio=image_size/img1.size[0]\n",
    "    else:\n",
    "        ratio=image_size/img1.size[1]\n",
    "    width=ratio*img1.size[0]\n",
    "    height=ratio*img1.size[1]\n",
    "    height_padding_size=int((image_size-height)/2)\n",
    "    width_padding_size=int((image_size-width)/2)\n",
    "    img1=np.array(img1.resize((int(width),int(height))))\n",
    "    train_x[index,height_padding_size:-height_padding_size,:] = img1\n",
    "# 훈련용 마스크 답지  및 병변여부 \n",
    "tf_train_x=tf.data.Dataset.from_tensor_slices(train_x)\n",
    "\n",
    "tf_train_x=(tf_train_x.map(normalize_img, num_parallel_calls=autotune).cache()\n",
    "    .batch(batch_size)).prefetch(tf.data.experimental.AUTOTUNE)\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KID(Kernel Inception Distance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KID(keras.metrics.Metric):\n",
    "    def __init__(self, name=\"kid\", **kwargs):\n",
    "        super().__init__(name=name, **kwargs)\n",
    "\n",
    "        # KID is estimated per batch and is averaged across batches\n",
    "        self.kid_tracker = keras.metrics.Mean()\n",
    "\n",
    "        # a pretrained InceptionV3 is used without its classification layer\n",
    "        # transform the pixel values to the 0-255 range, then use the same\n",
    "        # preprocessing as during pretraining\n",
    "        self.encoder = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                layers.Rescaling(255.0),\n",
    "                layers.Resizing(height=kid_image_size, width=kid_image_size),\n",
    "                layers.Lambda(keras.applications.inception_v3.preprocess_input),\n",
    "                keras.applications.InceptionV3(\n",
    "                    include_top=False,\n",
    "                    input_shape=(kid_image_size, kid_image_size, 3),\n",
    "                    weights=\"imagenet\",\n",
    "                ),\n",
    "                layers.GlobalAveragePooling2D(),\n",
    "            ],\n",
    "            name=\"inception_encoder\",\n",
    "        )\n",
    "\n",
    "    def polynomial_kernel(self, features_1, features_2):\n",
    "        feature_dimensions = tf.cast(tf.shape(features_1)[1], dtype=tf.float32)\n",
    "        return (features_1 @ tf.transpose(features_2) / feature_dimensions + 1.0) ** 3.0\n",
    "\n",
    "    def update_state(self, real_images, generated_images, sample_weight=None):\n",
    "        real_features = self.encoder(real_images, training=False)\n",
    "        generated_features = self.encoder(generated_images, training=False)\n",
    "\n",
    "        # compute polynomial kernels using the two sets of features\n",
    "        kernel_real = self.polynomial_kernel(real_features, real_features)\n",
    "        kernel_generated = self.polynomial_kernel(\n",
    "            generated_features, generated_features\n",
    "        )\n",
    "        kernel_cross = self.polynomial_kernel(real_features, generated_features)\n",
    "\n",
    "        # estimate the squared maximum mean discrepancy using the average kernel values\n",
    "        batch_size = tf.shape(real_features)[0]\n",
    "        batch_size_f = tf.cast(batch_size, dtype=tf.float32)\n",
    "        mean_kernel_real = tf.reduce_sum(kernel_real * (1.0 - tf.eye(batch_size))) / (\n",
    "            batch_size_f * (batch_size_f - 1.0)\n",
    "        )\n",
    "        mean_kernel_generated = tf.reduce_sum(\n",
    "            kernel_generated * (1.0 - tf.eye(batch_size))\n",
    "        ) / (batch_size_f * (batch_size_f - 1.0))\n",
    "        mean_kernel_cross = tf.reduce_mean(kernel_cross)\n",
    "        kid = mean_kernel_real + mean_kernel_generated - 2.0 * mean_kernel_cross\n",
    "\n",
    "        # update the average KID estimate\n",
    "        self.kid_tracker.update_state(kid)\n",
    "\n",
    "    def result(self):\n",
    "        return self.kid_tracker.result()\n",
    "\n",
    "    def reset_state(self):\n",
    "        self.kid_tracker.reset_state()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adaptive discriminator augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# \"hard sigmoid\", useful for binary accuracy calculation from logits\n",
    "def step(values):\n",
    "    # negative values -> 0.0, positive values -> 1.0\n",
    "    return 0.5 * (1.0 + tf.sign(values))\n",
    "\n",
    "\n",
    "# augments images with a probability that is dynamically updated during training\n",
    "class AdaptiveAugmenter(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        # stores the current probability of an image being augmented\n",
    "        self.probability = tf.Variable(0.0)\n",
    "\n",
    "        # the corresponding augmentation names from the paper are shown above each layer\n",
    "        # the authors show (see figure 4), that the blitting and geometric augmentations\n",
    "        # are the most helpful in the low-data regime\n",
    "        self.augmenter = keras.Sequential(\n",
    "            [\n",
    "                layers.InputLayer(input_shape=(image_size, image_size, 3)),\n",
    "                # blitting/x-flip:\n",
    "                layers.RandomFlip(\"horizontal\"),\n",
    "                # blitting/integer translation:\n",
    "                layers.RandomTranslation(\n",
    "                    height_factor=max_translation,\n",
    "                    width_factor=max_translation,\n",
    "                    interpolation=\"nearest\",\n",
    "                ),\n",
    "                # geometric/rotation:\n",
    "                layers.RandomRotation(factor=max_rotation),\n",
    "                # geometric/isotropic and anisotropic scaling:\n",
    "                layers.RandomZoom(\n",
    "                    height_factor=(-max_zoom, 0.0), width_factor=(-max_zoom, 0.0)\n",
    "                ),\n",
    "            ],\n",
    "            name=\"adaptive_augmenter\",\n",
    "        )\n",
    "\n",
    "    def call(self, images, training):\n",
    "        if training:\n",
    "            augmented_images = self.augmenter(images, training)\n",
    "\n",
    "            # during training either the original or the augmented images are selected\n",
    "            # based on self.probability\n",
    "            augmentation_values = tf.random.uniform(\n",
    "                shape=(batch_size, 1, 1, 1), minval=0.0, maxval=1.0\n",
    "            )\n",
    "            augmentation_bools = tf.math.less(augmentation_values, self.probability)\n",
    "\n",
    "            images = tf.where(augmentation_bools, augmented_images, images)\n",
    "        return images\n",
    "\n",
    "    def update(self, real_logits):\n",
    "        current_accuracy = tf.reduce_mean(step(real_logits))\n",
    "\n",
    "        # the augmentation probability is updated based on the dicriminator's\n",
    "        # accuracy on real images\n",
    "        accuracy_error = current_accuracy - target_accuracy\n",
    "        self.probability.assign(\n",
    "            tf.clip_by_value(\n",
    "                self.probability + accuracy_error / integration_steps, 0.0, 1.0\n",
    "            )\n",
    "        )"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Network architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DCGAN generator\n",
    "def get_generator():\n",
    "    noise_input = keras.Input(shape=(noise_size,))\n",
    "    x = layers.Dense(4 * 4 * width, use_bias=False)(noise_input)\n",
    "    x = layers.BatchNormalization(scale=False)(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape(target_shape=(4, 4, width))(x)\n",
    "    for _ in range(depth - 1):\n",
    "        x = layers.Conv2DTranspose(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.ReLU()(x)\n",
    "    image_output = layers.Conv2DTranspose(\n",
    "        3, kernel_size=4, strides=2, padding=\"same\", activation=\"sigmoid\",\n",
    "    )(x)\n",
    "\n",
    "    return keras.Model(noise_input, image_output, name=\"generator\")\n",
    "\n",
    "\n",
    "# DCGAN discriminator\n",
    "def get_discriminator():\n",
    "    image_input = keras.Input(shape=(image_size, image_size, 3))\n",
    "    x = image_input\n",
    "    for _ in range(depth):\n",
    "        x = layers.Conv2D(\n",
    "            width, kernel_size=4, strides=2, padding=\"same\", use_bias=False,\n",
    "        )(x)\n",
    "        x = layers.BatchNormalization(scale=False)(x)\n",
    "        x = layers.LeakyReLU(alpha=leaky_relu_slope)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(dropout_rate)(x)\n",
    "    output_score = layers.Dense(1)(x)\n",
    "\n",
    "    return keras.Model(image_input, output_score, name=\"discriminator\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GanModel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GAN_ADA(keras.Model):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "        self.augmenter = AdaptiveAugmenter()\n",
    "        self.generator = get_generator()\n",
    "        self.ema_generator = keras.models.clone_model(self.generator)\n",
    "        self.discriminator = get_discriminator()\n",
    "\n",
    "    def compile(self, generator_optimizer, discriminator_optimizer, **kwargs):\n",
    "        super().compile(**kwargs)\n",
    "\n",
    "        # separate optimizers for the two networks\n",
    "        self.generator_optimizer = generator_optimizer\n",
    "        self.discriminator_optimizer = discriminator_optimizer\n",
    "\n",
    "        self.generator_loss_tracker = keras.metrics.Mean(name=\"g_loss\")\n",
    "        self.discriminator_loss_tracker = keras.metrics.Mean(name=\"d_loss\")\n",
    "        self.real_accuracy = keras.metrics.BinaryAccuracy(name=\"real_acc\")\n",
    "        self.generated_accuracy = keras.metrics.BinaryAccuracy(name=\"gen_acc\")\n",
    "        self.augmentation_probability_tracker = keras.metrics.Mean(name=\"aug_p\")\n",
    "        self.kid = KID()\n",
    "\n",
    "    @property\n",
    "    def metrics(self):\n",
    "        return [\n",
    "            self.generator_loss_tracker,\n",
    "            self.discriminator_loss_tracker,\n",
    "            self.real_accuracy,\n",
    "            self.generated_accuracy,\n",
    "            self.augmentation_probability_tracker,\n",
    "            self.kid,\n",
    "        ]\n",
    "\n",
    "    def generate(self, batch_size, training):\n",
    "        latent_samples = tf.random.normal(shape=(batch_size, noise_size))\n",
    "        # use ema_generator during inference\n",
    "        if training:\n",
    "            generated_images = self.generator(latent_samples, training)\n",
    "        else:\n",
    "            generated_images = self.ema_generator(latent_samples, training)\n",
    "        return generated_images\n",
    "\n",
    "    def adversarial_loss(self, real_logits, generated_logits):\n",
    "        # this is usually called the non-saturating GAN loss\n",
    "\n",
    "        real_labels = tf.ones(shape=(batch_size, 1))\n",
    "        generated_labels = tf.zeros(shape=(batch_size, 1))\n",
    "\n",
    "        # the generator tries to produce images that the discriminator considers as real\n",
    "        generator_loss = keras.losses.binary_crossentropy(\n",
    "            real_labels, generated_logits, from_logits=True\n",
    "        )\n",
    "        # the discriminator tries to determine if images are real or generated\n",
    "        discriminator_loss = keras.losses.binary_crossentropy(\n",
    "            tf.concat([real_labels, generated_labels], axis=0),\n",
    "            tf.concat([real_logits, generated_logits], axis=0),\n",
    "            from_logits=True,\n",
    "        )\n",
    "\n",
    "        return tf.reduce_mean(generator_loss), tf.reduce_mean(discriminator_loss)\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        real_images = self.augmenter(real_images, training=True)\n",
    "\n",
    "        # use persistent gradient tape because gradients will be calculated twice\n",
    "        with tf.GradientTape(persistent=True) as tape:\n",
    "            generated_images = self.generate(batch_size, training=True)\n",
    "            # gradient is calculated through the image augmentation\n",
    "            generated_images = self.augmenter(generated_images, training=True)\n",
    "\n",
    "            # separate forward passes for the real and generated images, meaning\n",
    "            # that batch normalization is applied separately\n",
    "            real_logits = self.discriminator(real_images, training=True)\n",
    "            generated_logits = self.discriminator(generated_images, training=True)\n",
    "\n",
    "            generator_loss, discriminator_loss = self.adversarial_loss(\n",
    "                real_logits, generated_logits\n",
    "            )\n",
    "\n",
    "        # calculate gradients and update weights\n",
    "        generator_gradients = tape.gradient(\n",
    "            generator_loss, self.generator.trainable_weights\n",
    "        )\n",
    "        discriminator_gradients = tape.gradient(\n",
    "            discriminator_loss, self.discriminator.trainable_weights\n",
    "        )\n",
    "        self.generator_optimizer.apply_gradients(\n",
    "            zip(generator_gradients, self.generator.trainable_weights)\n",
    "        )\n",
    "        self.discriminator_optimizer.apply_gradients(\n",
    "            zip(discriminator_gradients, self.discriminator.trainable_weights)\n",
    "        )\n",
    "\n",
    "        # update the augmentation probability based on the discriminator's performance\n",
    "        self.augmenter.update(real_logits)\n",
    "\n",
    "        self.generator_loss_tracker.update_state(generator_loss)\n",
    "        self.discriminator_loss_tracker.update_state(discriminator_loss)\n",
    "        self.real_accuracy.update_state(1.0, step(real_logits))\n",
    "        self.generated_accuracy.update_state(0.0, step(generated_logits))\n",
    "        self.augmentation_probability_tracker.update_state(self.augmenter.probability)\n",
    "\n",
    "        # track the exponential moving average of the generator's weights to decrease\n",
    "        # variance in the generation quality\n",
    "        for weight, ema_weight in zip(\n",
    "            self.generator.weights, self.ema_generator.weights\n",
    "        ):\n",
    "            ema_weight.assign(ema * ema_weight + (1 - ema) * weight)\n",
    "\n",
    "        # KID is not measured during the training phase for computational efficiency\n",
    "        return {m.name: m.result() for m in self.metrics[:-1]}\n",
    "\n",
    "    def test_step(self, real_images):\n",
    "        generated_images = self.generate(batch_size, training=False)\n",
    "\n",
    "        self.kid.update_state(real_images, generated_images)\n",
    "\n",
    "        # only KID is measured during the evaluation phase for computational efficiency\n",
    "        return {self.kid.name: self.kid.result()}\n",
    "\n",
    "    def plot_images(self, epoch=None, logs=None, num_rows=3, num_cols=6, interval=25):\n",
    "        # plot random generated images for visual evaluation of generation quality\n",
    "        if epoch is None or (epoch + 1) % interval == 0:\n",
    "            num_images = num_rows * num_cols\n",
    "            generated_images = self.generate(num_images, training=False)\n",
    "            \n",
    "            plt.figure(figsize=(num_cols * 2.0, num_rows * 2.0))\n",
    "            for row in range(num_rows):\n",
    "                for col in range(num_cols):\n",
    "                    index = row * num_cols + col\n",
    "                    plt.subplot(num_rows, num_cols, index + 1)\n",
    "                    plt.imshow(generated_images[index])\n",
    "                    plt.axis(\"off\")\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            plt.close()\n",
    "        return generated_images\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 864x432 with 18 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "54/54 [==============================] - 182s 3s/step - g_loss: 2.9840 - d_loss: 0.0992 - real_acc: 0.9913 - gen_acc: 0.9497 - aug_p: 0.0021\n",
      "Epoch 2/50\n",
      "54/54 [==============================] - 219s 4s/step - g_loss: 4.8735 - d_loss: 0.0704 - real_acc: 0.9925 - gen_acc: 0.9902 - aug_p: 0.0062\n",
      "Epoch 3/50\n",
      "54/54 [==============================] - 218s 4s/step - g_loss: 4.9648 - d_loss: 0.0300 - real_acc: 0.9931 - gen_acc: 0.9994 - aug_p: 0.0100\n",
      "Epoch 4/50\n",
      "54/54 [==============================] - 208s 4s/step - g_loss: 7.6025 - d_loss: 0.0126 - real_acc: 0.9977 - gen_acc: 0.9994 - aug_p: 0.0141\n",
      "Epoch 5/50\n",
      "34/54 [=================>............] - ETA: 1:16 - g_loss: 7.8804 - d_loss: 0.0078 - real_acc: 0.9982 - gen_acc: 1.0000 - aug_p: 0.0175"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 19\u001b[0m\n\u001b[1;32m     17\u001b[0m img\u001b[39m=\u001b[39mmodel\u001b[39m.\u001b[39mplot_images()\n\u001b[1;32m     18\u001b[0m cv2\u001b[39m.\u001b[39mimwrite(\u001b[39m\"\u001b[39m\u001b[39m../data/d2/pred/ada_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.jpg\u001b[39m\u001b[39m\"\u001b[39m,img[\u001b[39m0\u001b[39m]\u001b[39m.\u001b[39mnumpy()\u001b[39m*\u001b[39m\u001b[39m255\u001b[39m)\n\u001b[0;32m---> 19\u001b[0m model\u001b[39m.\u001b[39;49mfit(\n\u001b[1;32m     20\u001b[0m tf_train_x,\n\u001b[1;32m     21\u001b[0m epochs\u001b[39m=\u001b[39;49mnum_epochs)\n\u001b[1;32m     22\u001b[0m model\u001b[39m.\u001b[39msave_weights(\u001b[39m\"\u001b[39m\u001b[39m../model/StyleGan/embryo_micro/model512_\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m+\u001b[39m\u001b[39mstr\u001b[39m(i)\u001b[39m+\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m.tf\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/utils/traceback_utils.py:64\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m filtered_tb \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     63\u001b[0m \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m---> 64\u001b[0m   \u001b[39mreturn\u001b[39;00m fn(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[1;32m     65\u001b[0m \u001b[39mexcept\u001b[39;00m \u001b[39mException\u001b[39;00m \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=broad-except\u001b[39;00m\n\u001b[1;32m     66\u001b[0m   filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/engine/training.py:1221\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1219\u001b[0m logs \u001b[39m=\u001b[39m tmp_logs  \u001b[39m# No error, now safe to assign to logs.\u001b[39;00m\n\u001b[1;32m   1220\u001b[0m end_step \u001b[39m=\u001b[39m step \u001b[39m+\u001b[39m data_handler\u001b[39m.\u001b[39mstep_increment\n\u001b[0;32m-> 1221\u001b[0m callbacks\u001b[39m.\u001b[39;49mon_train_batch_end(end_step, logs)\n\u001b[1;32m   1222\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstop_training:\n\u001b[1;32m   1223\u001b[0m   \u001b[39mbreak\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/callbacks.py:436\u001b[0m, in \u001b[0;36mCallbackList.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[39m\"\"\"Calls the `on_train_batch_end` methods of its callbacks.\u001b[39;00m\n\u001b[1;32m    430\u001b[0m \n\u001b[1;32m    431\u001b[0m \u001b[39mArgs:\u001b[39;00m\n\u001b[1;32m    432\u001b[0m \u001b[39m    batch: Integer, index of batch within the current epoch.\u001b[39;00m\n\u001b[1;32m    433\u001b[0m \u001b[39m    logs: Dict. Aggregated metric results up until this batch.\u001b[39;00m\n\u001b[1;32m    434\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    435\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_should_call_train_batch_hooks:\n\u001b[0;32m--> 436\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook(ModeKeys\u001b[39m.\u001b[39;49mTRAIN, \u001b[39m'\u001b[39;49m\u001b[39mend\u001b[39;49m\u001b[39m'\u001b[39;49m, batch, logs\u001b[39m=\u001b[39;49mlogs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/callbacks.py:295\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook\u001b[0;34m(self, mode, hook, batch, logs)\u001b[0m\n\u001b[1;32m    293\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_call_batch_begin_hook(mode, batch, logs)\n\u001b[1;32m    294\u001b[0m \u001b[39melif\u001b[39;00m hook \u001b[39m==\u001b[39m \u001b[39m'\u001b[39m\u001b[39mend\u001b[39m\u001b[39m'\u001b[39m:\n\u001b[0;32m--> 295\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_end_hook(mode, batch, logs)\n\u001b[1;32m    296\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m    297\u001b[0m   \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m    298\u001b[0m       \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mUnrecognized hook: \u001b[39m\u001b[39m{\u001b[39;00mhook\u001b[39m}\u001b[39;00m\u001b[39m. Expected values are [\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mbegin\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m, \u001b[39m\u001b[39m\"\u001b[39m\u001b[39mend\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m]\u001b[39m\u001b[39m'\u001b[39m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/callbacks.py:316\u001b[0m, in \u001b[0;36mCallbackList._call_batch_end_hook\u001b[0;34m(self, mode, batch, logs)\u001b[0m\n\u001b[1;32m    313\u001b[0m   batch_time \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_start_time\n\u001b[1;32m    314\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times\u001b[39m.\u001b[39mappend(batch_time)\n\u001b[0;32m--> 316\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_call_batch_hook_helper(hook_name, batch, logs)\n\u001b[1;32m    318\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_batch_times) \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_num_batches_for_timing_check:\n\u001b[1;32m    319\u001b[0m   end_hook_name \u001b[39m=\u001b[39m hook_name\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/callbacks.py:354\u001b[0m, in \u001b[0;36mCallbackList._call_batch_hook_helper\u001b[0;34m(self, hook_name, batch, logs)\u001b[0m\n\u001b[1;32m    352\u001b[0m \u001b[39mfor\u001b[39;00m callback \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallbacks:\n\u001b[1;32m    353\u001b[0m   hook \u001b[39m=\u001b[39m \u001b[39mgetattr\u001b[39m(callback, hook_name)\n\u001b[0;32m--> 354\u001b[0m   hook(batch, logs)\n\u001b[1;32m    356\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_check_timing:\n\u001b[1;32m    357\u001b[0m   \u001b[39mif\u001b[39;00m hook_name \u001b[39mnot\u001b[39;00m \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_hook_times:\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/callbacks.py:1032\u001b[0m, in \u001b[0;36mProgbarLogger.on_train_batch_end\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1031\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mon_train_batch_end\u001b[39m(\u001b[39mself\u001b[39m, batch, logs\u001b[39m=\u001b[39m\u001b[39mNone\u001b[39;00m):\n\u001b[0;32m-> 1032\u001b[0m   \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_batch_update_progbar(batch, logs)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/callbacks.py:1104\u001b[0m, in \u001b[0;36mProgbarLogger._batch_update_progbar\u001b[0;34m(self, batch, logs)\u001b[0m\n\u001b[1;32m   1100\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m add_seen\n\u001b[1;32m   1102\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mverbose \u001b[39m==\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[1;32m   1103\u001b[0m   \u001b[39m# Only block async when verbose = 1.\u001b[39;00m\n\u001b[0;32m-> 1104\u001b[0m   logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39;49msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1105\u001b[0m   \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mprogbar\u001b[39m.\u001b[39mupdate(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen, \u001b[39mlist\u001b[39m(logs\u001b[39m.\u001b[39mitems()), finalize\u001b[39m=\u001b[39m\u001b[39mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/utils/tf_utils.py:554\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type\u001b[0;34m(tensors)\u001b[0m\n\u001b[1;32m    551\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(x) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[39mreturn\u001b[39;00m t  \u001b[39m# Don't turn ragged or sparse tensors to NumPy.\u001b[39;00m\n\u001b[0;32m--> 554\u001b[0m \u001b[39mreturn\u001b[39;00m tf\u001b[39m.\u001b[39;49mnest\u001b[39m.\u001b[39;49mmap_structure(_to_single_numpy_or_python_type, tensors)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36mmap_structure\u001b[0;34m(func, *structure, **kwargs)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/tensorflow/python/util/nest.py:869\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    865\u001b[0m flat_structure \u001b[39m=\u001b[39m (flatten(s, expand_composites) \u001b[39mfor\u001b[39;00m s \u001b[39min\u001b[39;00m structure)\n\u001b[1;32m    866\u001b[0m entries \u001b[39m=\u001b[39m \u001b[39mzip\u001b[39m(\u001b[39m*\u001b[39mflat_structure)\n\u001b[1;32m    868\u001b[0m \u001b[39mreturn\u001b[39;00m pack_sequence_as(\n\u001b[0;32m--> 869\u001b[0m     structure[\u001b[39m0\u001b[39m], [func(\u001b[39m*\u001b[39;49mx) \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m entries],\n\u001b[1;32m    870\u001b[0m     expand_composites\u001b[39m=\u001b[39mexpand_composites)\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/keras/utils/tf_utils.py:550\u001b[0m, in \u001b[0;36msync_to_numpy_or_python_type.<locals>._to_single_numpy_or_python_type\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_to_single_numpy_or_python_type\u001b[39m(t):\n\u001b[1;32m    549\u001b[0m   \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(t, tf\u001b[39m.\u001b[39mTensor):\n\u001b[0;32m--> 550\u001b[0m     x \u001b[39m=\u001b[39m t\u001b[39m.\u001b[39;49mnumpy()\n\u001b[1;32m    551\u001b[0m     \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mitem() \u001b[39mif\u001b[39;00m np\u001b[39m.\u001b[39mndim(x) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39melse\u001b[39;00m x\n\u001b[1;32m    552\u001b[0m   \u001b[39mreturn\u001b[39;00m t\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1149\u001b[0m, in \u001b[0;36m_EagerTensorBase.numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[39m\"\"\"Copy of the contents of this Tensor into a NumPy array or scalar.\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \n\u001b[1;32m   1128\u001b[0m \u001b[39mUnlike NumPy arrays, Tensors are immutable, so this method has to copy\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1146\u001b[0m \u001b[39m    NumPy dtype.\u001b[39;00m\n\u001b[1;32m   1147\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m   1148\u001b[0m \u001b[39m# TODO(slebedev): Consider avoiding a copy for non-CPU or remote tensors.\u001b[39;00m\n\u001b[0;32m-> 1149\u001b[0m maybe_arr \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy()  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1150\u001b[0m \u001b[39mreturn\u001b[39;00m maybe_arr\u001b[39m.\u001b[39mcopy() \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(maybe_arr, np\u001b[39m.\u001b[39mndarray) \u001b[39melse\u001b[39;00m maybe_arr\n",
      "File \u001b[0;32m~/anaconda3/envs/LeeYS/lib/python3.8/site-packages/tensorflow/python/framework/ops.py:1115\u001b[0m, in \u001b[0;36m_EagerTensorBase._numpy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39m_numpy\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m   1114\u001b[0m   \u001b[39mtry\u001b[39;00m:\n\u001b[0;32m-> 1115\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_numpy_internal()\n\u001b[1;32m   1116\u001b[0m   \u001b[39mexcept\u001b[39;00m core\u001b[39m.\u001b[39m_NotOkStatusException \u001b[39mas\u001b[39;00m e:  \u001b[39m# pylint: disable=protected-access\u001b[39;00m\n\u001b[1;32m   1117\u001b[0m     \u001b[39mraise\u001b[39;00m core\u001b[39m.\u001b[39m_status_to_exception(e) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model = GAN_ADA()\n",
    "model.compile(\n",
    "    generator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    "    discriminator_optimizer=keras.optimizers.Adam(learning_rate, beta_1),\n",
    ")\n",
    "\n",
    "# save the best model based on the validation KID metric\n",
    "checkpoint_path = \"../model/StyleGan/embryo_micro/model_{epoch:03d}.tf\"\n",
    "checkpoint_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path,\n",
    "    monitor=\"val_kid\",\n",
    "    mode=\"min\",\n",
    ")\n",
    "\n",
    "# run training and plot generated images periodically\n",
    "for i in range(100):\n",
    "    img=model.plot_images()\n",
    "    cv2.imwrite(\"../data/d2/pred/ada_\"+str(i)+\".jpg\",img[0].numpy()*255)\n",
    "    model.fit(\n",
    "    tf_train_x,\n",
    "    epochs=num_epochs)\n",
    "    model.save_weights(\"../model/StyleGan/embryo_micro/model512_\"+str(i)+\".tf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "LeeYS",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "ace6096bc4ec7c9763b7a7f8aba72601b4924b997cc8a285eb70b9b69117778c"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
